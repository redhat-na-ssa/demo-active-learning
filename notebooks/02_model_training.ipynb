{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bf0069f-335c-42e5-8bb9-e45d051f95a9",
   "metadata": {},
   "source": [
    "# Multiclass Vegetable Classification Using Transfer Learning\n",
    "\n",
    "This notebook uses a Keras InceptionV3 model, which is trained on more than 1M images from ImageNet database, and uses transfer learning for a new task of classifying vegetables from images.\n",
    "\n",
    "The source of this notebook came from [Vegetable Classification Using Transfer Learning on Kaggle](https://www.kaggle.com/code/theeyeschico/vegetable-classification-using-transfer-learning) using the Vegetable Image Dataset and is an excellent source to learn this and other tutorials.\n",
    "\n",
    ">Citation: Ahmed, M. Israk & Mamun, Shahriyar & Asif, Asif. (2021). DCNN-Based Vegetable Image Classification Using Transfer Learning: A Comparative Study. 235-243. 10.1109/ICCCSP52374.2021.9465499. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc104e6-e18d-4d67-b31b-b56b320c1fbe",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d01b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# scratch directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../scratch\n",
    "scratch_path = os.environ.get(\"SCRATCH\", \"scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd5585-23bd-4a12-b71a-1c8d3cc310e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "%pip install -q -U pip\n",
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d696475-c96e-4081-b892-fe9083e74b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "from PIL import Image \n",
    "from PIL import ImageEnhance\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, random, pathlib, warnings, itertools, math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61392b87-67fd-41d9-ae1b-d3e9f901c496",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81c07f-97b7-4a8b-ae5e-cb7dc199b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup paths for data\n",
    "dataset = scratch_path + \"/Vegetable Images/\"\n",
    "\n",
    "train_folder = os.path.join(dataset, \"train\")\n",
    "test_folder = os.path.join(dataset, \"validation\")\n",
    "validation_folder = os.path.join(dataset, \"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8064823c-4c78-4a7b-bbd9-500547ca034e",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71d99b8a-35ce-4b22-a7bf-5c3839b1e9bc",
   "metadata": {},
   "source": [
    "Counting number of images in a folder. (test set in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26487c5-e629-48f8-ba7b-b38026d85c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(rootdir):\n",
    "    '''counts the number of files in each subfolder in a directory'''\n",
    "    for path in pathlib.Path(rootdir).iterdir():\n",
    "        if path.is_dir():\n",
    "            print(\"There are \" + str(len([name for name in os.listdir(path) \\\n",
    "            if os.path.isfile(os.path.join(path, name))])) + \" files in \" + \\\n",
    "            str(path.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86426a1-ee40-4376-aac7-0d6003cf3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_files(os.path.join(train_folder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "191429e8-b7d5-49d8-a871-d739877a4166",
   "metadata": {},
   "source": [
    "Counting number of images in a folder. (test set in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4a55f-a7e8-4cb1-8998-284dd9320898",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_files(os.path.join(test_folder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f86dee3b-8b7f-4298-a432-a8e574dceeba",
   "metadata": {},
   "source": [
    "As evident, Dataset is well balanced with each class containing :\n",
    "1000 images for training set.\n",
    "200 images for test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b41735e-a12d-44a6-b87d-19441290e512",
   "metadata": {},
   "source": [
    "# Image Proprocessing\n",
    "\n",
    "The goal of image processing is improvement of pictorial information for human interpretation. Basic manipulation and filtering can lead to increased understanding for feature extraction as well.\n",
    "\n",
    "We increase the color saturation, contrast and finally sharpened the image for drawing texture and viewer focus. The image after processing looks appealing and brighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff16343-eef5-438d-930b-22da3f634016",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder=\"Cucumber\"  # The vegetable you want to display\n",
    "number_of_images=2       # Number of images to display\n",
    "\n",
    "def Preprocess():\n",
    "    j=1\n",
    "    for i in range(number_of_images):\n",
    "    \n",
    "        folder = os.path.join(test_folder,image_folder)\n",
    "        a=random.choice(os.listdir(folder))\n",
    "\n",
    "        image=Image.open(os.path.join(folder,a))\n",
    "        image_duplicate=image.copy()\n",
    "        plt.figure(figsize=(10,10))\n",
    "\n",
    "        plt.subplot(number_of_images,2,j)\n",
    "        plt.title(label='Orignal', size=17, pad='7.0', loc=\"center\", fontstyle='italic')\n",
    "        plt.imshow(image)\n",
    "        j+=1\n",
    "\n",
    "        image1=ImageEnhance.Color(image_duplicate).enhance(1.35)\n",
    "        image1=ImageEnhance.Contrast(image1).enhance(1.45)\n",
    "        image1=ImageEnhance.Sharpness(image1).enhance(2.5)\n",
    "        \n",
    "        plt.subplot(number_of_images,2,j)\n",
    "        plt.title(label='Processed', size=17, pad='7.0', loc=\"center\", fontstyle='italic')\n",
    "        plt.imshow(image1)\n",
    "        j+=1\n",
    "        \n",
    "Preprocess()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70ebd0be-1f9b-49b9-b4b9-cb4895a6e6af",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "We can start exploring the dataset and visualize any class label (for instance, Capsicum). You can choose any vegetable to visualize the images of that class. Changing rows and columns variable also results in different format positioning of matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458cc35-6352-4ef3-8e0b-f0360ef943f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_vegetable=\"Capsicum\"\n",
    "rows,columns = 1,5\n",
    "\n",
    "display_folder=os.path.join(train_folder,select_vegetable)\n",
    "total_images=rows*columns\n",
    "fig=plt.figure(1, figsize=(20, 10))\n",
    "\n",
    "for i,j in enumerate(os.listdir(display_folder)):      \n",
    "    \n",
    "    img = plt.imread(os.path.join(train_folder,select_vegetable,j))\n",
    "    fig=plt.subplot(rows, columns, i+1)\n",
    "    fig.set_title(select_vegetable, pad = 11, size=20)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    if i==total_images-1:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92f79533-9dae-406e-b3e0-2ad3c2b14e75",
   "metadata": {},
   "source": [
    "Now let's visualize the whole dataset by picking a random image from each class inside training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8252dd1-5725-49c2-9805-24f8fffc8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "\n",
    "for food_folder in sorted(os.listdir(train_folder)):\n",
    "    food_items = os.listdir(train_folder + '/' + food_folder)\n",
    "    food_selected = np.random.choice(food_items)\n",
    "    images.append(os.path.join(train_folder,food_folder,food_selected))\n",
    "                                     \n",
    "fig=plt.figure(1, figsize=(15, 10))\n",
    "\n",
    "for subplot,image_ in enumerate(images):\n",
    "    category=image_.split('/')[-2]\n",
    "    imgs = plt.imread(image_)\n",
    "    a,b,c=imgs.shape\n",
    "    fig=plt.subplot(3, 5, subplot+1)\n",
    "    fig.set_title(category, pad = 10,size=18)\n",
    "    plt.imshow(imgs)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f4e152e-bad1-4de9-8971-246435a9e43b",
   "metadata": {},
   "source": [
    "There are 15 vegetables (output classes) and one random image from each class helps in determining basic outlook of dataset and what picture quality along with different metric are visible. So far, So Good!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d6486d-32ac-498c-ad9e-8dbc5b323eb8",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac33fb7-7caa-4957-afbd-7ebfe2a506fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batches of tensor image data with real-time data augmentation.\n",
    "train_datagen = image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Takes the path to a directory & generates batches of augmented data.\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    train_folder, target_size=(224, 224), batch_size=64, class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# Generate batches of tensor image data with real-time data augmentation.\n",
    "test_datagen = image.ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Takes the path to a directory & generates batches of augmented data.\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    test_folder, target_size=(224, 224), batch_size=64, class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "class_map = train_dataset.class_indices\n",
    "print(class_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "783dde4a-371d-4d92-8385-da7839dfb815",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "Let's start building the transfer learning network to train our model using InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea0ca7-e7da-428e-bb15-4f0c0a6f6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "inception = InceptionV3(\n",
    "    input_shape=IMAGE_SIZE + [3], weights=\"imagenet\", include_top=False\n",
    ")\n",
    "\n",
    "for layer in inception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = inception.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "prediction = Dense(15, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inception.input, outputs=prediction)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822b2fb-6797-4e7a-91b0-a12a0e4bfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = train_dataset.class_indices\n",
    "class_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ff419a4-aab5-4125-bd66-38aac4ba1ab7",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff593ae4-9b92-4498-b3e8-4885c257b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata = \"_inceptionV3_epoch5\"\n",
    "path_to_model = scratch_path + \"/models/model\" + model_metadata + \".h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3b34660-bacb-48fa-b398-0e9f4c135eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 650s 3s/step - loss: 0.0407 - accuracy: 0.9872 - val_loss: 0.0281 - val_accuracy: 0.9907\n",
      "Epoch 2/5\n",
      "235/235 [==============================] - 454s 2s/step - loss: 0.0274 - accuracy: 0.9907 - val_loss: 0.0236 - val_accuracy: 0.9923\n",
      "Epoch 3/5\n",
      "235/235 [==============================] - 368s 2s/step - loss: 0.0269 - accuracy: 0.9909 - val_loss: 0.0106 - val_accuracy: 0.9963\n",
      "Epoch 4/5\n",
      "235/235 [==============================] - 332s 1s/step - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.0168 - val_accuracy: 0.9963\n",
      "Epoch 5/5\n",
      "182/235 [======================>.......] - ETA: 1:16 - loss: 0.0235 - accuracy: 0.9928"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(path_to_model):\n",
    "    print(\"Model: Training...\")\n",
    "    history = model.fit_generator(\n",
    "        train_dataset,\n",
    "        validation_data=test_dataset,\n",
    "        epochs=5,\n",
    "        steps_per_epoch=len(train_dataset),\n",
    "        validation_steps=len(test_dataset)\n",
    "        )\n",
    "    print(\"Model: Trained\")\n",
    "\n",
    "    print(\"Model: Saving...\")\n",
    "    model.save(path_to_model)\n",
    "    print(\"Model: Saved\")\n",
    "\n",
    "else:\n",
    "    print(\"Model: Already Exists\")\n",
    "    model = load_model(path_to_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1eca007a-9b40-4b8f-90e3-a87e9bcc4412",
   "metadata": {},
   "source": [
    "## Plot Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d671a-8740-4c6c-8205-90e8e34c74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    \n",
    "    plt.plot(history.history['accuracy'],label='train accuracy')\n",
    "    plt.plot(history.history['val_accuracy'],label='validation accuracy')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(scratch_path + 'Accuracy_v1_InceptionV3')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_loss(history):\n",
    "    \n",
    "    plt.plot(history.history['loss'],label=\"train loss\")\n",
    "    plt.plot(history.history['val_loss'],label=\"validation loss\")\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(scratch_path + 'Loss_v1_InceptionV3')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9812d-6372-479c-a800-cab9610636f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_plot = scratch_path + '/Accuracy_v1_InceptionV3.png'\n",
    "loss_plot = scratch_path + '/Loss_v1_InceptionV3.png'\n",
    "\n",
    "# display previously saved plots\n",
    "\n",
    "try:\n",
    "    display(Image(filename=acc_plot))\n",
    "except:\n",
    "    print(\"Scoring: ...\")\n",
    "    plot_accuracy(history)\n",
    "\n",
    "try:\n",
    "    display(Image(filename=loss_plot))\n",
    "except:\n",
    "    print(\"Scoring: ...\")\n",
    "    plot_loss(history)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ce1f94b-0e9a-4b8e-9a7a-89896a4fb263",
   "metadata": {},
   "source": [
    "## Visualize Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ab678-4331-4cf5-b9aa-f6f1c4ca3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total layers in the model : \",len(model.layers),\"\\n\")\n",
    "\n",
    "layers = [layer.output for layer in model.layers[0:]]\n",
    "layer_names = []\n",
    "for layer in model.layers[0:]: \n",
    "    layer_names.append(layer.name)\n",
    "    \n",
    "print(\"First layer : \", layer_names[0])\n",
    "print(\"InceptionV3 layers : Layer 2 to Layer 311\")\n",
    "print(\"Our fine tuned layers : \", layer_names[311:314])\n",
    "print(\"Final Layer : \", layer_names[314])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d23cdb55-18aa-44d0-b97d-7ff4ea2404d9",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "750d3bb5-c9fd-4b64-8495-e6d159747bba",
   "metadata": {},
   "source": [
    "## Load the model from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61e391-33af-451b-bb1a-f4660e3e70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "print(\"Model: Loading...\")\n",
    "model = load_model(path_to_model)\n",
    "print(\"Model: Loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e77abeb8-5a1c-4a07-8afb-aed685651e60",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7241b09-d837-447f-b604-504ba1c618c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_dir = scratch_path + '/Vegetable Images/test'\n",
    "\n",
    "validation_datagen = image.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical')\n",
    "\n",
    "scores = model.evaluate_generator(validation_generator)\n",
    "print(\"Test Accuracy: {:.3f}\".format(scores[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21eb26fe-1485-4c84-bb4a-58fc10e7af7c",
   "metadata": {},
   "source": [
    "## Confusion Matrix for evaluating the performance of our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5e9e9-930c-44a9-b9c3-a82d923fc59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_confusion_matrix(validation_folder):\n",
    "    \n",
    "    folder_path=validation_folder\n",
    "    \n",
    "    mapping={}\n",
    "    for i,j in enumerate(sorted(os.listdir(folder_path))):\n",
    "        mapping[j]=i\n",
    "    \n",
    "    files=[]\n",
    "    real=[]\n",
    "    predicted=[]\n",
    "\n",
    "    for i in os.listdir(folder_path):\n",
    "        \n",
    "        true=os.path.join(folder_path,i)\n",
    "        true=true.split('/')[-1]\n",
    "        true=mapping[true]\n",
    "        \n",
    "        for j in os.listdir(os.path.join(folder_path,i)):\n",
    "\n",
    "            img_ = image.load_img(os.path.join(folder_path,i,j), target_size=(224,224))\n",
    "            img_array = image.img_to_array(img_)\n",
    "            img_processed = np.expand_dims(img_array, axis=0) \n",
    "            img_processed /= 255.\n",
    "            prediction = model.predict(img_processed)\n",
    "            index = np.argmax(prediction)\n",
    "\n",
    "            predicted.append(index)\n",
    "            real.append(true)\n",
    "            \n",
    "    return (real,predicted)\n",
    "\n",
    "def print_confusion_matrix(real,predicted):\n",
    "    total_output_labels = 15\n",
    "    cmap=\"turbo\"\n",
    "    cm_plot_labels = [i for i in range(15)]\n",
    "    \n",
    "    cm = confusion_matrix(y_true=real, y_pred=predicted)\n",
    "    df_cm = pd.DataFrame(cm,cm_plot_labels,cm_plot_labels)\n",
    "    sns.set(font_scale=1.2) # for label size\n",
    "    plt.figure(figsize = (15,10))\n",
    "    s=sns.heatmap(df_cm,fmt=\"d\", annot=True,cmap=cmap) # font size\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(scratch_path + '/confusion_matrix.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807548d-aa1a-456f-ad2f-6bfe8166335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "y_true,y_pred=labels_confusion_matrix(validation_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ff2a7-e397-4b96-8ea5-c8f94dfc1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO make this an if else to load the .png if a model exists\n",
    "print_confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66806501-572b-4ef0-8d14-ef02a7916c58",
   "metadata": {},
   "source": [
    "The false positives are really low as we have used transfer learning which has given us good accuracy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad7fbb25-6f12-4fed-a311-3bed38a404b1",
   "metadata": {},
   "source": [
    "# Prediction in Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5accc0b-b92a-48c1-960b-1f816a29589d",
   "metadata": {},
   "source": [
    "## Function to predict output of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5415e7-98eb-41df-81e8-7d369158bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "category={\n",
    "    0: 'Bean', 1: 'Bitter_Gourd', 2: 'Bottle_Gourd', 3 : 'Brinjal', 4: \"Broccoli\", 5: 'Cabbage', 6: 'Capsicum', 7: 'Carrot', 8: 'Cauliflower',\n",
    "    9: 'Cucumber', 10: 'Papaya', 11: 'Potato', 12: 'Pumpkin', 13 : \"Radish\", 14: \"Tomato\"\n",
    "}\n",
    "\n",
    "def predict_image(filename,model):\n",
    "    img_ = image.load_img(filename, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img_)\n",
    "    img_processed = np.expand_dims(img_array, axis=0) \n",
    "    img_processed /= 255.   \n",
    "    \n",
    "    prediction = model.predict(img_processed)\n",
    "    index = np.argmax(prediction)\n",
    "    \n",
    "    plt.title(\"Prediction - {}\".format(category[index]))\n",
    "    plt.imshow(img_array)\n",
    "    \n",
    "def predict_dir(filedir, model):\n",
    "    cols=3\n",
    "    pos=0\n",
    "    images=[]\n",
    "    total_images=len(os.listdir(filedir))\n",
    "    rows=total_images//cols + 1\n",
    "    \n",
    "    true=filedir.split('/')[-1]\n",
    "    \n",
    "    for i in sorted(os.listdir(filedir)):\n",
    "        images.append(os.path.join(filedir,i))\n",
    "        \n",
    "    for subplot, imggg in enumerate(images):\n",
    "        img_ = image.load_img(imggg, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img_)\n",
    "        img_processed = np.expand_dims(img_array, axis=0) \n",
    "        img_processed /= 255.\n",
    "        prediction = model.predict(img_processed)\n",
    "        index = np.argmax(prediction)\n",
    "        \n",
    "        pred=category.get(index)\n",
    "        if pred==true:\n",
    "            pos+=1\n",
    "\n",
    "    acc=pos/total_images\n",
    "    print(\"Accuracy for {orignal}: {:.2f} ({pos}/{total})\".format(acc,pos=pos,total=total_images,orignal=true))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed53cc9e-de32-4f19-be16-a622947decba",
   "metadata": {},
   "source": [
    "## Single image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc506ef-2b37-4ed5-811d-932820d09ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(os.path.join(validation_folder,'Cauliflower/1064.jpg'),model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17a9fa84-5990-46d8-b451-a42907bfa7c3",
   "metadata": {},
   "source": [
    "## Directory of images predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9a616c8-8fdf-4568-8e90-904a66977fc9",
   "metadata": {},
   "source": [
    "#TODO this works, but need to cleanup output\n",
    "for i in os.listdir(test_folder):\n",
    "    predict_dir(os.path.join(test_folder,i),model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3863cc74-2835-4964-8b48-2752dfd7a5c5",
   "metadata": {},
   "source": [
    "# TODO option to cleanup output\n",
    "from IPython.utils import io\n",
    "\n",
    "for i in os.listdir(test_folder):\n",
    "    with io.capture_output() as captured:\n",
    "        predict_dir(os.path.join(test_folder, i), model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1965928f-3a1d-4a39-825d-9f2519fdc551",
   "metadata": {},
   "source": [
    "## Visualize incorrect predictions\n",
    "Visualizing all incorrect images predicted for a particular vegetable category by our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af708ab-efff-43a5-b202-d6034283cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_input_capture(test_category):\n",
    "    \n",
    "    a=os.path.basename(test_category)\n",
    "    wrong_array=[]\n",
    "    \n",
    "    for i in os.listdir(test_category):\n",
    "        \n",
    "        imggg=os.path.join(test_category,i)\n",
    "        \n",
    "        img_ = image.load_img(imggg, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img_)\n",
    "        img_processed = np.expand_dims(img_array, axis=0) \n",
    "        img_processed /= 255.\n",
    "        prediction = model.predict(img_processed)\n",
    "        \n",
    "        index = np.argmax(prediction)\n",
    "        pred=category.get(index)\n",
    "        \n",
    "        if not pred==a:\n",
    "            wrong_array.append((imggg,pred))\n",
    "            \n",
    "    return wrong_array\n",
    "    \n",
    "def visualize_wrong_input(images):\n",
    "    \n",
    "    fig=plt.figure(1, figsize=(20, 25))\n",
    "    total_images=len(images)\n",
    "    rows=math.ceil(float(total_images/3))\n",
    "    for subplot,(image_path,predicted) in enumerate(images):\n",
    "        img = plt.imread(image_path)\n",
    "        fig=plt.subplot(rows, 3, subplot+1)\n",
    "        fig.set_title(\"Predicted - {}\".format(predicted), pad = 10,size=18)\n",
    "        plt.imshow(img)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784aa234-6e62-46b2-b8ee-a9aadfcbe381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "vegetable=\"Broccoli\"\n",
    "path=os.path.join(validation_folder,vegetable)\n",
    "images= wrong_input_capture(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92a129-c160-452f-b2ed-25d15a651064",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_wrong_input(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d488aae-4777-4644-ac91-94742f5e0f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
